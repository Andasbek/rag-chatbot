# RAG Chatbot — Разговор с PDF-документом

Этот проект — простой пример **RAG-чатбота** (Retrieval-Augmented Generation), который умеет:

- загружать **один PDF-документ**;
- индексировать его содержимое (разбивать на фрагменты, преобразовывать в эмбеддинги, сохранять во векторное хранилище);
- отвечать на вопросы **строго на основе текста PDF**;
- если в документе нет нужной информации — честно писать об этом, **без галлюцинаций**.

Проект сделан на Python с использованием **LangChain 0.3+**, `langchain-openai` и `Chroma` как векторного хранилища.

---

## 1. Стек технологий

- Язык: **Python 3.11+**
- LLM: **OpenAI GPT-4o** (или любой другой, доступный через `langchain-openai`)
- Эмбеддинги: **OpenAIEmbeddings**
- Векторное хранилище: **Chroma** (`langchain-chroma`)
- Фреймворк для RAG: **LangChain 0.3+**
- Формат документа: **PDF**

---

## 2. Структура проекта

```text
rag-chatbot/
├─ data/
│  └─ docs/               # сюда кладутся PDF-документы
│     └─ sample.pdf       # пример: статья для теста
├─ src/
│  ├─ __init__.py         # делает src Python-пакетом
│  ├─ config.py           # пути, настройки чанкинга, загрузка .env
│  ├─ loaders.py          # загрузка и парсинг PDF
│  ├─ splitter.py         # разбиение текста на чанки
│  ├─ vectorstore.py      # создание и загрузка векторного хранилища Chroma
│  ├─ rag_chain.py        # сборка RAG-цепочки (retriever + LLM + промпт)
│  └─ main.py             # точка входа: индексация и CLI-чат
├─ .env                   # OPENAI_API_KEY=...
├─ requirements.txt       # зависимости проекта
├─ .gitignore
└─ README.md
````

---

## 3. Установка и запуск

### 3.1. Клонирование репозитория

```bash
git clone <url-репозитория>
cd rag-chatbot
```

### 3.2. Виртуальное окружение (рекомендуется)

```bash
python -m venv venv
source venv/bin/activate      # macOS / Linux
# или
venv\Scripts\activate         # Windows
```

### 3.3. Установка зависимостей

```bash
pip install -r requirements.txt
```

### 3.4. Настройка переменных окружения

Создай файл `.env` в корне проекта:

```env
OPENAI_API_KEY=sk-...
```

Где `sk-...` — твой OpenAI API ключ.

---

## 4. Подготовка PDF-документа

1. Помести свой PDF-файл в папку `data/docs/`, например:

   ```text
   data/docs/sample.pdf
   ```

2. Убедись, что имя файла совпадает с тем, которое используется в `main.py`:

   ```python
   prepare_index("sample.pdf")
   ```

---

## 5. Как это работает изнутри

### 5.1. Шаг 1 — Загрузка PDF (`loaders.py`)

Используем `PyPDFLoader` для извлечения текста из PDF:

* Каждый элемент списка `docs` — это `Document` с полями:

  * `page_content` — текст страницы/фрагмента;
  * `metadata` — метаданные (например, номер страницы).

### 5.2. Шаг 2 — Чанкинг (`splitter.py`)

Документ разбивается на небольшие фрагменты (чанки):

* `chunk_size = 1000` символов;
* `chunk_overlap = 200` символов (перекрытие, чтобы не «резать» смысл).

Это помогает LLM работать с релевантным контекстом и не выходить за лимиты токенов.

### 5.3. Шаг 3 — Векторное хранилище (`vectorstore.py`)

1. Каждый чанк превращается в вектор через `OpenAIEmbeddings`.

2. Все векторы сохраняются в **Chroma**:

   ```python
   vs = Chroma.from_documents(
       chunks,
       embedding=embeddings,
       persist_directory="data/chroma_db"
   )
   ```

3. При последующих запусках векторное хранилище просто загружается из `data/chroma_db`.

### 5.4. Шаг 4 — RAG-цепочка (`rag_chain.py`)

1. Загружается векторное хранилище, создаётся `retriever`.

2. Настраивается строгий `system_prompt`, который:

   * запрещает галлюцинации;
   * заставляет использовать только контекст из документа;
   * требует честно писать:

     > «В документе нет информации, достаточной для ответа на этот вопрос.»
     > если данных не хватает.

3. RAG-цепочка собирается как пайплайн:

   * вход: `question` (вопрос пользователя);
   * `retriever` находит релевантные чанки → `format_docs()` превращает их в единый контекст;
   * контекст и вопрос подставляются в `ChatPromptTemplate`;
   * `ChatOpenAI` генерирует ответ;
   * `StrOutputParser` возвращает строку.

### 5.5. Шаг 5 — CLI-чат (`main.py`)

* Функция `prepare_index(pdf_name)`:

  * загружает PDF;
  * разбивает на чанки;
  * создаёт векторное хранилище.
* Функция `chat()`:

  * создаёт RAG-цепочку;
  * в цикле читает вопросы из консоли;
  * печатает ответ и номера страниц, которые использовались.

---

## 6. Запуск: индексация + чат

По умолчанию в `main.py` стоит:

```python
if __name__ == "__main__":
    # Для отладки: каждый запуск пересобирает индекс и сразу запускает чат
    prepare_index("sample.pdf")
    chat()
```

### 6.1. Запуск (отладочный режим — индексация каждый раз)

```bash
python -m src.main
```

В консоли ожидаешь примерно:

```text
[MAIN] === НАЧАЛО ИНДЕКСАЦИИ ДЛЯ ФАЙЛА: sample.pdf ===
[LOAD_PDF] Пытаюсь загрузить файл: .../data/docs/sample.pdf
[LOAD_PDF] Загружено страниц/документов: 4
[SPLITTER] Число чанков после разбиения: 23
[VECTORSTORE] Создаю Chroma в .../data/chroma_db
[VECTORSTORE] Векторное хранилище создано (auto-persist).
[MAIN] === ИНДЕКСАЦИЯ ЗАВЕРШЕНА ===
[VECTORSTORE] Загружаю Chroma из .../data/chroma_db
[VECTORSTORE] Количество записей в коллекции: 23
RAG-бот запущен. Введите вопрос (или 'exit'):
```

Теперь можно задавать вопросы:

```text
>> Что такое искусственный интеллект?
>> Расскажи, какие преимущества авторы связывают с внедрением ИИ.
>> Какие риски или ограничения использования ИИ упоминаются в статье?
```

---

## 7. Режим «индекс один раз»

Когда всё работает стабильно, можно:

1. Один раз вызвать индексацию:

   ```python
   if __name__ == "__main__":
       prepare_index("sample.pdf")
       # chat()
   ```

2. После успешной индексации закомментировать `prepare_index` и оставить только чат:

   ```python
   if __name__ == "__main__":
       # prepare_index("sample.pdf")
       chat()
   ```

3. Запускать:

   ```bash
   python -m src.main
   ```

И векторная база будет использоваться повторно.

---

## 8. Гарантия «без галлюцинаций»

Поведение бота контролируется через `system_prompt`:

* модель **не имеет права** отвечать, если в контексте нет нужной информации;
* при недостатке данных она обязана вывести:

  > «В документе нет информации, достаточной для ответа на этот вопрос.»

Это соответствует требованиям типичного учебного проекта по RAG: не выдавать придуманные факты, а честно признавать отсутствие данных в источнике.

---

## 9. Возможные направления развития

Если захочется расширить проект, можно:

* заменить CLI на **веб-интерфейс**:

  * FastAPI + простая HTML/JS страничка;
  * или React / Next.js фронтенд;
* добавить **загрузку PDF из UI** (через форму);
* подключить **несколько документов** (RAG по коллекции статей);
* добавить логирование вопросов/ответов;
* добавить режим «объясни, из каких абзацев документа ты сделал вывод» (показ chunk’ов целиком).

---

## 10. Полезные команды

```bash
# Установка зависимостей
pip install -r requirements.txt

# Активация виртуального окружения
source venv/bin/activate        # macOS / Linux
venv\Scripts\activate           # Windows

# Удаление старой базы Chroma (если нужно пересобрать индекс)
rm -rf data/chroma_db

# Запуск проекта
python -m src.main
```

---

Если используешь этот проект в рамках учебного задания по RAG, можно в отчёте/презентации кратко описать:

* что такое RAG;
* как устроен pipeline (PDF → чанки → эмбеддинги → Chroma → retriever → LLM);
* пример вопросов:

  * вопрос, на который документ даёт ответ;
  * вопрос на обобщение;
  * вопрос, на который документа не хватает (и бот честно отвечает, что данных нет).